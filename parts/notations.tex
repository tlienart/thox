% !TEX root = ../thesis.tex

\chapter*{Notations}
\pagenumbering{roman}
\setcounter{page}{1}

We list here notations used throughout the document that are considered to be commonly used in computational statistics. Non-standard notations will be introduced in the document explicitly.

\subsection*{Vectors, matrices and functions} 
For a vector $x\in\R^{d}$ we write $\norm{x}{p}$ the $p$-norm of $x$ with $\norm{x}{p}^{p}=\sum_{i=1:d}\abs{x_{i}}^{p}$ for $p\in[1,\infty)$. 
For $x,y\in\R^{d}$, we write $\scal{x,y}=\sum_{i=1:d}x_{i}y_{i}$ the inner product of $x$ and $y$. For a nonnegative vector $x_u$ we write $x_u\propto x$ if there is a normalisation constant $Z_x$ such that $x=Z_x\inv x_u$ has components summing to one.
We denote the transpose of a matrix $A\in\R^{d\times d}$ by $A\transp$. $A$ is said to be semi positive definite if $\scal{x,Ax}\ge 0$ for all $x\in\R^{d}$ and positive definite if the inequality holds strictly for all $x\neq 0$. We denote by $\mathbb S_+^{d}\subset \mathbb R^{d\times d}$ the set of symmetric positive definite matrices and by $\mathbb S_-^{d}$ the set of matrices $B$ such that $-B\in\mathbb S_+^{d}$.
For a real-valued measurable function $f$ on a set $\mathcal X\subseteq \mathbb R^{d}$, we write $\int f(x)\mathrm d{x}$ to denote the integral of $f$ on the whole of $\mathcal X$. We write $\|f\|_{p}^{p}=\int |f(x)|^{p}\dx$ the $p$-norm of $f$ for $p\in[1,\infty)$. 
We write $L^{1}(\mathcal X)$ the set of functions such that $\|f\|_{1}<\infty$ and $\mathcal P(\mathcal X) \subset L^{1}(\mathcal X)$ denotes the restriction to nonnegative functions integrating to one (probability density functions). For a nonnegative function $g_u\in  L^{1}(\mathcal X)$ we write $g_u\propto g$ to indicate that there is a normalisation constant $Z_g$ such that $g=Z_g\inv g_u$ with $g\in\mathcal P(\mathcal X)$.  
For a real-valued, differentiable function $f$ on $\mathcal X$, we write $\nabla f$ its gradient and $\nabla^{2} f$ its Hessian provided $f$ is twice differentiable. \check{june30, jun21,may10}
%
\subsection*{Probabilities and expected values}
We consider continuous sample spaces $\mathcal X\subseteq \R^{d}$ and, by default, we consider the associated Borel $\sigma$-algebra. 
Correspondingly, we use the shorthand $(\mathcal X,\P)$ to denote a probability space with $\P$ the probability measure. 
We consider probability measures that admit a probability density function $p$ with respect to a base measure $\nu$ with therefore $\P(x\in A)=\int_{A}p(x)\nudx$ for any open subset $A\subset \mathcal X$. We write $X\sim p$ to denote a random variable associated to $p$ and $\{X^{(i)}\}_{i=1}^{N}\simiid p$ for a set of independent such random variables.
In particular, $\mathcal N(\cdot; \mu,\Sigma)$ denotes the multivariate normal distribution with mean $\mu\in \R^{d}$ and covariance matrix $\Sigma\in\mathbb S^{d}_+$, $\delta_a(\cdot)$ denotes the Dirac-delta distribution with point-mass at $a$ and $\mathcal M(\alpha_{1},\dots,\alpha_{d})$ is the multinomial distribution over $d$ cases.
For such a probability space $(\mathcal X, \mathbb P)$ and a vector-valued mapping $\varphi:\mathcal X\to\R^{d}$, we write the expectation and the covariance matrix of $\varphi$ with respect to $p$ as $(\E_{p}[\varphi(X)])_{i}=\int_{\mathcal X}\varphi_{i}(x)p(x)\nudx$ and $\V_{p}[\varphi(X)]=\E_{p}[\varphi(X)\varphi(X)^{t}]-\E_{p}[\varphi(X)]\E_{p}[\varphi(X)]^{t}$ respectively. 
We write $\KL{p,q}=\E_{p}[\log p(X)]-\E_{p}[\log q(X))]$ the Kullback-Leibler divergence between two probability distribution functions $p$ and $q$ on $\mathcal X$. \check{jun21,june15,june8,april27}
%

\subsection*{Convex analysis}
Let $B\subset\R^{n}$ denote the Euclidean unit ball with $B=\{x\in\R^{n}\mid \|x\|_{2}\le 1\}$. For any subset $C\subseteq\R^{n}$, we write $\mathrm{int}\,(C)$ the interior of $C$ with $\mathrm{int}\,(C)=\{x\in C \mid \exists \epsilon>0, x+\epsilon B \subset C\}$. Correspondingly we write $\mathrm{cl}(C)$ the closure of $C$ with $\mathrm{cl}(C) = \cap\{C+\epsilon B\st \epsilon>0\}$. A set $C$ is convex if for any $x,y\in C$, $(1-\lambda)x+\lambda y\in C$ for all $\lambda\in(0,1)$. Let $\Omega$ be an arbitrary connected and nonempty subset of $\R^{n}$; a function $f:\Omega\to\R$ is said to be convex if $f((1-\lambda)x+\lambda y)\le (1-\lambda)f(x)+\lambda f(y)$. For an arbitrary function $f:\Omega\to\R$, we denote by $f^{\star}:\R\to \R\cup\{+\infty\}$ the convex-conjugate of $f$ with $f^{\star}(y)=\sup_{y\in \Omega}[\scal{x,y}-f(x)]$. The set of minimisers of a function $f$ on a open, nonempty set $C$ is denoted by $\arg\min_{x\in C}\,f(x)$ i.e. the set $\pab{x\in C\mid f(y)\ge f(x), \,\forall y\in C}$. For a differentiable, strictly convex function $\varphi:C\to\R$, we denote by $B_\varphi(x,y)$ the Bregman divergence associated to $\varphi$ between $x$ and $y$ in $C$ with $B_\varphi(x,y)=\varphi(x)-\varphi(y)-\scal{x-y,\nabla \varphi(y)}$.  \check{jun21,may13}

\subsection*{Miscellaneous}
We denote an undirected graph by a set of vertex labels $\mathcal V$ and edges $\mathcal E\subseteq \mathcal V\times\mathcal V$. For a node $u\in\mathcal V$ we write $\partial u = \{v \in\mathcal V \st (u,v)\in \mathcal E\}$ the neighbourhood of node $u$.
%\subsection*{Complexity and convergence}

We denote the complexity of an algorithm by $\mathcal O(h(N))$ to indicate that it scales in $N$ like $h(N)$ for large $N$. If the algorithm is $\mathcal O(N)$ (resp.\ $\mathcal O(N^{2})$) we say the algorithm has linear (resp.\ quadratic) complexity in $N$. If the complexity is between linear and quadratic, e.g.: $\mathcal O(N\log N)$, we say the algorithm is sub-quadratic. We use the same notations to describe the convergence algorithms.


%If the function $f$ is convex and differentiable on $C$ we call \emph{first-order condition} the fact that this set is equivalent to the set of points $x\in C$ such that $\nabla f(x)=0$.

%\dred{note} 
%\add{explain notations for message, explain add comma for t-1,t}
%
%\setlength{\tabcolsep}{12pt}
%\renewcommand{\arraystretch}{1.2}
%\begin{tabular}{ll}
%\textbf{symbol} 		& \textbf{meaning}\\[.3cm]
%$\mathcal X$ 			& sample space\\
%$x,y,z \in \mathcal X$	& sampling points\\
%$X,Y,Z$ 				& random variables\\
%$x_{1:T}$ 				& collection $\{x_{t}\}_{t=1}^{T}$\\
%$X \sim q(\cdot)$ 		& random variable drawn from a distribution $q$\\
%$\mathcal N(\,\cdot\,; \mu, \Sigma)$ 
%						& Normal distribution with mean $\mu$ and covariance matrix $\Sigma$\\
%%$p(x_{1:T})$ & joint distribution over the collection $\{x_{t}\}_{t=1}^{T}$\\
%$p(x\st y)$ 				& conditional distribution at $x$ given that $y$ was observed\\
%$\text{supp}(p)$ 		& set of $x$ such that $p(x)>0$ where $p$ is a distribution\\
%$\delta_{a}(x)$ 		& Dirac-delta distribution with point mass at $x=a$\\
%$\hat p(x)$ 				& \dred{particle estimator of the distribution $p$}\\
%%$\pd_t(x_t)$ & \dred{(in HMM) predictive density $p(x_t\st y_{1:t-1})$}\\
%%$\bif_t(x_t)$ & \dred{(in HMM) backward information filter $p(y_{t:T}\st x_t)$}\\
%%$\tbif_t (x_t)$ & \dred{(in HMM) normalised backward information filter $\gamma_t(x_t)p(y_{t:T}\st x_t)$}\\
%$\psi_{u}$ 				& singleton potential on a node $u$\\
%$\psi_{uv}$ 				& pairwise potential on an edge between a node $u$ and a node $v$\\
%$\partial i$ 			& neighbourhood of node $i$ in a graph\\
%$m_{st}$ 				& message from node $s$ to node $t$\\
%$M_{st}$ 				& \dred{pre-message at $s$ going to $t$ in Belief Propagation}\\
%$\KL{p,q}$ 				& Kullback-Leibler divergence between two distributions $p$ and $q$\\
%$\mathcal F_\phi$ 		& exponential family with sufficient statistic $\phi$\\
%$q_\theta$ 				& distribution parametrised by $\theta$\\
%$A(\nu)$ 				& log-partition function of an $\mathcal F_{\phi}$ distribution parametrised by $\theta$\\
%$A^{\star}(\nu)$		& \dred{convex-conjugate of $A$}\\
%%$\theta[q]$ & natural parameter of distribution $q\in \mathcal F_{\phi}$\\
%%$\mu[q]$ & mean parameter of distribution $q\in \mathcal F_{\phi}$\\
%\dred{$\mathcal P_\phi[p]$} 	& Kullback-Leibler projection of a distribution $p$ on $\mathcal F_\phi$\\
%$\E_p[\phi]$ 			& expected value of $\phi(X)$ if $X\sim p(\cdot)$\\
%$\V_{p}[\phi]$ 			& \dred{variance of $\phi(X)$} if $X\sim p(\cdot)$  
%
%\end{tabular}

\setlength{\tabcolsep}{6pt} % default
\subsection*{Abbreviations}
We list below abbreviations that are used in this document by alphabetical order and link to where they are first used or introduced in the document.
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.2}
\begin{longtable}{llc}
BIF 			& Backward Information Filter &(section \ref{sec:TFS})\\
%BB$\alpha$	& BlackBox $\alpha$-divergence minimisation algorithm\\
BP/LBP 		& (Loopy) Belief Propagation &(section \ref{bg:belief-propag})\\
BPS/LBPS 	& (Local) Bouncy Particle Sampler & (section \ref{point:BPS})\\
EP	 		& Expectation Propagation &(section \ref{s:ADF+EP})\\
EPBP 		& Expectation Particle Belief Propagation &(section \ref{sec:EPBP})\\
ESS 			& Effective Sample Size &(section \ref{sec:MC+SMC})\\
FFBS 		& Forward Filtering Backward Smoothing &(section \ref{sec:MC+SMC})\\
IPP 			& Inhomogeneous Poisson Process &(section \ref{point:BPS})\\
KL 			& Kullback-Leibler divergence & (section \ref{sec:ABI})\\
GMMC/LMMC	& Local/Global Moment Matching Conditions &(section \ref{s:ADF+EP})\\
HMM 		& Hidden Markov Model &(section \ref{intro:exMRF})\\
MCMC 		& Markov Chain Monte Carlo &(section \ref{sec:MC+SMC})\\
MRF 		& Markov Random Field &(section \ref{subsection: mrf})\\
NBP 			& Nonparametric Belief Propagation &(section \ref{sec:LBPonCS})\\
PBP	 		& Particle Belief Propagation &(section \ref{sec:LBPonCS})\\
PD 			& Predictive Density &(section \ref{intro:exMRF})\\
PDMP 		& Piecewise Deterministic Markov Processes & (section \ref{point:BPS})\\
RMS(E) 		& Root Mean Squared (Error) &(section \ref{sec:TFSexp})\\
SMC 		& Sequential Monte Carlo &(section \ref{sec:MC+SMC})\\
SMS 		& Sampling via Moment Sharing &(section \ref{sec:ep-for-dbi})\\
%SEP 			& Stochastic EP\\
SNEP		& Stochastic Natural gradient EP &(section \ref{sec:ep-for-dbi})\\
TFS 			& Two Filter Smoothing &(section \ref{sec:TFS})\\ 
\end{longtable}
\setlength{\tabcolsep}{6pt} % default
\newpage
