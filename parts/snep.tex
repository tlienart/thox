% !TEX root = ../thesis.tex

\todofr{
	\begin{itemize}
		\item Geometry and KL and mirror descent
		\item trick of the trade when projections don't work and why
	\end{itemize}
}

%%%%%%%%%%%%%%%%%
\section{Context}
\todofr{
\begin{itemize}
	\item distributed bayesian inference various stuff
	\item using EP for distributed stuff
	\item (just mention, explain after) SMS (also Turner's work, SEP etc)
	\item mention here noise
\end{itemize}
}

In this section we are interested in performing Bayesian inference in the situation where the data is distributed across different compute nodes. 
Let us assume that there are $K$ such compute nodes each holding independent parts of the data $y_i$ (with $i=1,\dots,K$) such that these parts form a partition of the entire data $y$. 
In a Bayesian setting, we are interested in the posterior distribution over a parameter of interest $x$ given the entire data $y$. We can write this posterior as the product of $K$ terms corresponding to the parts $y_i$:
%
\eqa{
	p(x\st y) &\propto& p_0(x) \prod_{i=1}^{K} p(y_i\st x)
}
%
where $p_0$ is a prior distribution. Each of the likelihood term itself factors \add{COMPLETE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EP variants for distributed inference}
\todofr{
The sectioning has to be given a bit of thinking: should probably discuss updates in NP and MP space 
\begin{itemize}
	\item Natural 
	\item EP Energy (SMS and SNEP)
\end{itemize}
}
\subsection{\dred{SMS algorithm}}
\subsubsection*{Noisy EP iterations in natural parameter space}
We showed \hyperref[s:ADF+EP]{(point \ref*{s:ADF+EP})} that the basic Expectation Propagation algorithm can be understood as a kind of fixed point iteration targeting the Local Moment Matching Conditions (LMMC) \eqref{eq:LMMC}.
At a given step of the algorithm, when considering the inclusion of the factor $t_i$, the global parameter is updated in such a way that $\E_{q_\theta}[\phi(X)]= \E_{q_i}[\phi(X)]$. Let $\mu_i:=\E_{q_i}[\phi(X)]$, the update then amounts to finding $\theta$ such that $\nabla A(\theta) = \mu_i$ or
%
\eqa{	
	\theta &\leftarrow & \nabla A^{\star}(\mu_i).	\label{eq:ep-np-update}
}
%
In the case where one is considering a Monte Carlo estimator for $\mu_i$, these updates are inexact and it is crucial to mitigate the effect of the noise by considering \emph{damped updates}. In fact, even when one uses the exact $\mu_i$, it can be beneficial to the overall convergence of the algorithm to consider damped updates \dred{CITE XXX probably SMS}. Damping \eqref{eq:ep-np-update} leads to the modified update:\check{may11}
%
\eqa{
	\theta &\leftarrow & (1-\kappa)\theta + \kappa \nabla A^{\star}(\mu_i),\label{eq:damped-update-np}
}
%
where $\kappa \in(0,1]$ is the damping parameter. As in the gradient descent algorithm, the damping parameter can be decreased over iterations according to a schedule in order to further help convergence.\add{We will explore the connection to GD in more details later}
Writing $\theta=\lambda_i+\omega_i$ with $\omega_i$ the parameter of the current factor approximation $\tilde t_i$ and $\lambda_i=(\theta-\omega_i)$ the parameter corresponding to the cavity, the damped update \eqref{eq:damped-update-np} can be expressed as
%
\eqa{
	\omega_i &\leftarrow& \omega_i - \kappa(\omega_i+\lambda_i - \nabla A^{\star}(\mu_i)).
}
%
This form is useful in the parallel context since it can be interpreted in terms of a local update on one compute node corresponding to the factor $t_i$.\add{make sure it connects with context, say this better}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{The SMS algorithm}
\todofr{describe SMS algorithm in full} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\dred{SNEP algorithm}}
\subsubsection*{Noisy EP iterations in mean parameter space}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{The SNEP algorithm}

\subsection{\dred{The ``EP energy'' perspective}}
\todofr{
Cite minka, mention energy, explain that it's not really something to increase or decrease but rather want stationary points and that's it, show that if write the KL and get rid of a part then get this energy (essentially use that to show that it's a bit pointless)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Obtaining the energy}
Another perspective to EP is to consider the so-called ``EP Energy''\add{citations}. This ``energy'' is more a function that has the same fixed points as EP than an energy. One way to obtain it is to decompose the KL divergence between $p$ and $q$. First, let us write explicitly the objects considered:
\begin{itemize}
	\item the target distribution $p$ with $p(x)=Z_p\inv \pi_0(x)\prod_{i=1:K}t_i(x)$,
	\item the global approximating distribution $q$ with $q(x)=Z_q\inv \pi_0(x)\exp\scal{\theta,\phi(x)}$ which we can also write $Z_q\inv \pi_0(x)\exp\scal{\sum_{i=1:K}\omega_i,\phi(x)}$ as in \eqref{eq:approxadf},
	\item the tilted distributions $q_i$ with $q_i(x)=Z_i\inv \pi_0(x)\exp\scal{\lambda_i,\phi(x)}$.
\end{itemize}
In the list above, $Z_p,Z_q$ and $Z_i$ designate normalisation constants (of course, $Z_q=\exp A(\theta)$) and $\pi_0$ is the prior distribution. The parameters of the tilted distributions $\lambda_i$ are given by $\lambda_i=(\theta-\omega_i)$ and hence must be such that $\sum_{i=1:K}\lambda_i=(K-1)\theta$. To simplify notations, we omit the dependence in $x$ and all sums and products are assumed to go over $i=1,\dots,K$ and write
%
\eqa{	
	\KL{p,q} &=& \E_p\pac{\log\pat{Z_p\inv\pi_0\prod_{i}t_i}-\log q}	.
\label{eq:decomposeKL}}
%	
We can manipulate the product $t_i$ to make the $q_i$ appear:
%
\eqa{
	\prod_{i} t_i &=& \prod_{i}{\pi_0\exp\scal{\lambda_i,\phi}t_i\over \pi_0\exp\scal{\lambda_i,\phi}}\spe {\prod_i Z_iq_i\over \pi_0 (Z_q q)^{K-1}.}
}
%
Using this in \eqref{eq:decomposeKL} leads to\footnote{Note that if we consider the reverse KL we can write a very similar decomposition:
\[\KL{q,p} - \log Z_p \spe -\mathcal E + \sum_i \KL{q,q_i}.\]}
%
\eqa{
\KL{p,q} + \log Z_p &=& \underbrace{(1-K)\log Z_q + \sum_{i}\log Z_i}_{\mathcal E} + \mathbb E_p\pac{\log q_i/q}.
}
%
The first part, denoted by a $\mathcal E$ is the EP energy. Its gradient in $\lambda_i$ readily obtained:
%
\eqa{
	\nabla_{\lambda_i}\mathcal E &=& (1-K)\nabla_{\lambda_i}A\pat{(K-1)\inv \sum_i\lambda_i} + {\nabla_{\lambda_i}\int \pi_0(x)\exp\scal{\lambda_i,\phi(x)}t_i(x)\dx\over Z_i} \nn\\
	&=& -\nabla A(\theta) + \E_{q_i}[\phi].
}
%
The stationary points of $\mathcal E$ in $\lambda_i$ therefore correspond to the LMMC \eqref{eq:LMMC} that the EP algorithm attempts to satisfy.
It is also interesting to briefly look at the remainder of the right-hand side of \eqref{eq:decomposeKL}. Discarding the terms that don't depend on $\lambda_i$, we have
%
\eqa{
	\nabla_{\lambda_i}\E_p[\log q_i/q] &=& \nabla_{\lambda_i}\E_p\pac{\scal{\lambda_i,\phi}-\log Z_i  - \scal{(K-1)\inv\sum_i\lambda_i,\phi}+ \log Z_q}\nn\\
	&=& \mu-\mu_i-{1\over K-1}\mu + {1\over K-1}\nabla A(\theta).
}
%
Therefore the gradient of the complete right hand side of \eqref{eq:decomposeKL} (i.e. the gradient of $\KL{p,q}$) in $\lambda_i$ is in fact proportional to $(\mu-\nabla A(\theta))$ leading to the same stationary points as the GMMC.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Analysis of the EP energy}
\todofr{Think first about how you want to present information. Would be good not to go along the crappy road of the dev a la Teh, just showing that we're actually doing some kind of Newton's method would already be quite nice use for this \url{http://homes.soic.indiana.edu/classes/spring2012/csci/b553-hauserk/newtons_method.pdf} and probably ``Information geometry of mirror descent'' \citep{raskutti15}}


We know that the stationary points of $\mathcal E$ in $\lambda_i$ correspond to the LMMC. We can now seek to characterise this energy function in more details as a function of the $\lambda_i$ and in order to follow the literature, we will consider $-\mathcal E$. We also introduce the notation $A_i(\lambda_i) = \log Z_i$ so that the energy can be written as
\eqa{-\mathcal E(\lambda_1,\dots,\lambda_K) &=& \eta A\left(\eta\inv \sum_i\lambda_i\right) - \sum_i A_i(\lambda_i),}
with $\eta:=(K-1)$.\add{the function is not convex, not concave either, cool. See if useful to even write it like this}

\subsection{\dred{Parameter tying, SEP and BBa}}

%%%%%%%%%%%%%%%%%%%%%
\section{Comparisons}
\todofr{Explain primary comparison between SMS and SNEP since parameter tying is a further assumption (or say something of the sorts following Distbayes paper). Redo the experiments from ages ago (see if still has notebooks) comparing different EP approaches for BLR on single machine, discuss. Then discuss in distributed setting (graph from Distbayes paper)}


%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\todofr{
\begin{itemize}
	\item pitfalls of using EP / VB for large dimensional, have to revert to diagonal gaussians in which case maybe not so interesting. As a result and since we are using a large number of points, the uncertainty recovered is often extremely small in both EP and VB and unusable in practice. One may therefore wonder whether a purely optimisation based approach effectively targeting the MAP directly is not all that we can afford etc.
\end{itemize}
}