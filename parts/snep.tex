% !TEX root = ../thesis.tex

\todofr{
	\begin{itemize}
		\item Geometry and KL and mirror descent
		\item trick of the trade when projections don't work and why
	\end{itemize}
}

%%%%%%%%%%%%%%%%%
\section{Context}
\todofr{
\begin{itemize}
	\item distributed bayesian inference various stuff
	\item using EP for distributed stuff
	\item (just mention, explain after) SMS (also Turner's work, SEP etc)
	\item mention here noise
\end{itemize}
}

In this section we are interested in performing Bayesian inference in the situation where the data is distributed across different compute nodes. 
Let us assume that there are $K$ such compute nodes each holding independent parts of the data $y_i$ (with $i=1,\dots,K$) such that these parts form a partition of the entire data $y$. 
In a Bayesian setting, we are interested in the posterior distribution over a parameter of interest $x$ given the entire data $y$. We can write this posterior as the product of $K$ terms corresponding to the parts $y_i$:
%
\eqa{
	p(x\st y) &\propto& p_0(x) \prod_{i=1}^{K} p(y_i\st x)
}
%
where $p_0$ is a prior distribution. Each of the likelihood term itself factors \add{COMPLETE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EP variants for distributed inference}
\todofr{
The sectioning has to be given a bit of thinking: should probably discuss updates in NP and MP space 
\begin{itemize}
	\item Natural 
	\item EP Energy (SMS and SNEP)
\end{itemize}
}
\subsection{\dred{SMS algorithm}}
\subsubsection*{Noisy EP iterations in natural parameter space}
We showed \hyperref[s:ADF+EP]{(point \ref*{s:ADF+EP})} that the basic Expectation Propagation algorithm can be understood as a kind of fixed point iteration targeting the Local Moment Matching Conditions (LMMC) \eqref{eq:LMMC}.
At a given step of the algorithm, when considering the inclusion of the factor $t_i$, the global parameter is updated in such a way that $\E_{q_\theta}[\phi(X)]= \E_{q_i}[\phi(X)]$. Let $\mu_i:=\E_{q_i}[\phi(X)]$, the update then amounts to finding $\theta$ such that $\nabla A(\theta) = \mu_i$ or
%
\eqa{	
	\theta &\leftarrow & \nabla A^{\star}(\mu_i).	\label{eq:ep-np-update}
}
%
In the case where one is considering a Monte Carlo estimator for $\mu_i$, these updates are inexact and it is crucial to mitigate the effect of the noise by considering \emph{damped updates}. In fact, even when one uses the exact $\mu_i$, it can be beneficial to the overall convergence of the algorithm to consider damped updates \dred{CITE XXX probably SMS}. Damping \eqref{eq:ep-np-update} leads to the modified update:\check{may11}
%
\eqa{
	\theta &\leftarrow & (1-\kappa)\theta + \kappa \nabla A^{\star}(\mu_i),\label{eq:damped-update-np}
}
%
where $\kappa \in(0,1]$ is the damping parameter. As in the gradient descent algorithm, the damping parameter can be decreased over iterations according to a schedule in order to further help convergence.\add{We will explore the connection to GD in more details later}
Writing $\theta=\lambda_i+\omega_i$ with $\omega_i$ the parameter of the current factor approximation $\tilde t_i$ and $\lambda_i=(\theta-\omega_i)$ the parameter corresponding to the cavity, the damped update \eqref{eq:damped-update-np} can be expressed as
%
\eqa{
	\omega_i &\leftarrow& \omega_i - \kappa(\omega_i+\lambda_i - \nabla A^{\star}(\mu_i)).
}
%
This form is useful in the parallel context since it can be interpreted in terms of a local update on one compute node corresponding to the factor $t_i$.\add{make sure it connects with context, say this better}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{The SMS algorithm}
\todofr{describe SMS algorithm in full} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\dred{SNEP algorithm}}
\subsubsection*{Noisy EP iterations in mean parameter space}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{The SNEP algorithm}

\subsection{\dred{The ``EP energy'' perspective}}
\todofr{
Cite minka, mention energy, explain that it's not really something to increase or decrease but rather want stationary points and that's it, show that if write the KL and get rid of a part then get this energy (essentially use that to show that it's a bit pointless)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Obtaining the energy}
Another perspective to EP is to consider the so-called ``EP Energy''\add{citations}. This ``energy'' is more a function that has the same fixed points as EP than an energy. One way to obtain it is to decompose the KL divergence between $p$ and $q$. First, let us write explicitly the objects considered:
\begin{itemize}
	\item the target distribution $p$ with $p(x)=Z_p\inv \pi_0(x)\prod_{i=1:K}t_i(x)$,
	\item the global approximating distribution $q$ with $q(x)=Z_q\inv \pi_0(x)\exp\scal{\theta,\phi(x)}$ which we can also write $Z_q\inv \pi_0(x)\exp\scal{\sum_{i=1:K}\omega_i,\phi(x)}$ as in \eqref{eq:approxadf},
	\item the tilted distributions $q_i$ with $q_i(x)=Z_i\inv \pi_0(x)\exp\scal{\lambda_i,\phi(x)}$.
\end{itemize}
In the list above, $Z_p,Z_q$ and $Z_i$ designate normalisation constants and $\pi_0$ is the prior distribution. The parameters of the tilted distributions $\lambda_i$ are given by $\lambda_i=(\theta-\omega_i)$ and hence must be such that $\sum_{i=1:K}\lambda_i=(K-1)\theta$. Omitting to write the dependence in $x$ explicitly since it is clear from the context, we can write
%
\eqa{	
	\KL{p,q} &=& \E_p\pac{\log\pat{Z_p\inv\pi_0\prod_{i=1:K}t_i}-\log q}	.
\label{eq:decomposeKL}}
%	
We can manipulate the product $t_i$ to make the $q_i$ appear:
%
\eqa{
	\prod_{i=1:K} t_i &=& \prod_{i=1:K}{\pi_0\exp\scal{\lambda_i,\phi}t_i\over \pi_0\exp\scal{\lambda_i,\phi}}\spe {\prod Z_iq_i\over \pi_0 (Z_q q)^{K-1}.}
}
%
Using this in \eqref{eq:decomposeKL} leads to
%
\eqa{
\KL{p,q} + \log Z_p &=& \underbrace{(1-K)\log Z_q + \sum_{i=1:K}\log Z_i}_{\mathcal E} + \mathbb E_p\pac{\log q_i/q}.
}
%
The first part, denoted by a $\mathcal E$ is the EP energy. 

\subsection{\dred{Parameter tying, SEP and BBa}}

%%%%%%%%%%%%%%%%%%%%%
\section{Comparisons}
\todofr{Explain primary comparison between SMS and SNEP since parameter tying is a further assumption (or say something of the sorts following Distbayes paper). Redo the experiments from ages ago (see if still has notebooks) comparing different EP approaches for BLR on single machine, discuss. Then discuss in distributed setting (graph from Distbayes paper)}


%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\todofr{
\begin{itemize}
	\item pitfalls of using EP / VB for large dimensional, have to revert to diagonal gaussians in which case maybe not so interesting. As a result and since we are using a large number of points, the uncertainty recovered is often extremely small in both EP and VB and unusable in practice. One may therefore wonder whether a purely optimisation based approach effectively targeting the MAP directly is not all that we can afford etc.
\end{itemize}
}