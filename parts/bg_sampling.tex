% !TEX root = ../thesis.tex

\section{Monte Carlo and Sequential Monte Carlo}
\subsection{From quadrature to sampling}
We consider the problem of computing the expected value of a test function $\varphi$ with respect to a distribution $p\in\mathcal P(\mathcal X)$ i.e.: $I:=\E_p[\varphi(X)]$. Assuming it can't be computed analytically, a general approach is to consider a quadrature rule of the form:\add{be consistent with $n$ number of points $p$ dimensions etc.}
%
\eqa{
	I\esp\approx\esp \widehat I_n &=& \sum_{i=1}^{n} w(X^{(i)}) \varphi(X^{(i)})
}
%
for some points $X^{(i)}\in\mathcal X$ and corresponding weights $w(X^{(i)})\in\mathbb R_+$. 
When the number of dimensions is low, we can consider deterministic quadrature rules such as the Gauss-Hermite quadrature. 
However, as the dimensionality increases, the performances of these deterministic rules become catastrophic even for a large number of integration points.\footnote{An effect related to what Bellman called the ``curse of dimensionality'' in dynamic programming \citep{bellman57}.} 
In such cases, a broadly studied approach is to consider Monte Carlo integration with
%
\eqa{	
	X^{(i)}\esp \simiid\esp p,\quad\text{and}\quad w(X^{(i)})\spe n^{-1}. 	
}
%
The strong law of large numbers then indicates that $\widehat I_n\to I$ almost surely with approximation error scaling like $\mathcal O(n^{-1/2})$ independently of the dimensionality of the problem. This is to be compared with deterministic rules which typically have approximation error scaling like $\mathcal O(n^{-\alpha/d})$ for a fixed $\alpha>0$ depending on the quadrature rule \citep{caflisch98}. The problem then becomes one of drawing iid.\ samples from $p$ which is often also an intractable problem.

%%%%%%%
\subsection{Sequential importance sampling}
For this point, we refer to the note by \citet{doucet11} and also to \citet[chapter 3.3 and 14.3]{robert04}.

In \emph{importance sampling} (IS)\margnote{IS}, samples are drawn from a \emph{proposal distribution}\margnote{proposal} $q$ that is easy to sample from (for example, a Gaussian) and is similar to $p$. 
The quadrature weights are then adjusted to reflect that the samples are not drawn from the true distribution:
%
\eqa{
	X^{(i)}\esp \simiid\esp q(\cdot), \quad\text{and}\quad w(X^{(i)})\spe {p(X^{(i)})\over n q(X^{(i)})}.
} 
%
Provided the support of $q$ englobes that of $p$ i.e., $p(x)>0\Rightarrow q(x)>0$, the resulting \emph{importance sampling estimator} is consistent.\check{june21apr1}

In the context of HMM, we are often interested in estimating a sequence of target distributions $\{p_{t}(x_{1:t})\}_{t=1}^{T}$ which can be decomposed as 
%
\eqa{
	p_{t}(x_{1:t}) &=& p_{t}(x_{t}\st x_{1:t-1})p_{t-1}(x_{1:t-1}).\nn
}
%
This lead to the development of \emph{sequential Monte Carlo} (SMC) methods. The principle is the same as that of importance sampling except that a different proposal is considered at every time $t$ taking into account the previous draw of particles and the evolution of the system:
%
\eqa{
	q_{t}(x_{1:t}) &=& q_{t}(x_{t}\st x_{1:t-1})q_{t-1}(x_{1:t-1}).	\nn
}
%
Following this form, new samples or \emph{particles} $X^{(i)}_t$ can be drawn from $q_{t}(x_{t}\st X^{(i)}_{1:t-1})$ and the weights corresponding to the trajectories $\{X^{(i)}_{1:t}\}$ then need to be updated by a factor $\alpha^{(i)}_{t}$ with
%
\eqa{
	\alpha^{(i)}_{t} := {\pi_{t}(X^{(i)}_{1:t} )\over \pi_{t-1}(X^{(i)}_{1:t-1}) q_{t}(X^{(i)}_{t}\st X^{(i)}_{1:t-1})}.
}
%
The variance of an IS estimator is directly related to the variance of the importance weights. In order to reduce the increase of variance induced by the sequential IS procedure, the proposal at time $t$ should be such that the variance of the update factors $\alpha_t$ is small. In particular, the \emph{optimal proposal} keeps it at zero with
\eqa{		q^{\text{opt}}_{t}(x_{t}\st x_{1:t-1}) &:=& \pi_{t}(x_{t}\st x_{1:t-1}).	\label{optimal proposal}}
However, we can't typically sample easily from the transition density and consequently have to resort to approximating distributions.\add{check transition density introduced} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\dred{Particle filtering}}
In the filtering problem\add{check introduced earlier, link} on a HMM, the target densities are $\pi_{t}(x_{1:t})=p(x_{1:t}\st y_{1:t})$ and their marginals. The incremental update factors are given by
\eqa{ \alpha_t(x_{1:t}) &=& {p(x_{1:t}\st y_{1:t})\over p(x_{1:t-1}\st y_{1:t-1})q_t(x_t\st x_{1:t-1})} 	\nn\\
	&=& {p(x_t,y_t\st x_{1:t-1},y_{1:t-1})p(x_{1:t-1}\st y_{1:t-1})p(y_{1:t-1})\over p(x_{1:t-1}\st y_{1:t-1})q_t(x_t\st x_{1:t-1})p(y_{1:t})}  \nn\\
	&\propto& p(y_t\st x_t)p(x_t\st x_{t-1}) }
Where we exploited the conditional dependence structure of a HMM. Consequently, the optimal proposal is
\eqa{		q^{\text{opt}}_{t}(x_{t}\st x_{t-1}) &\propto& p(y_{t}\st x_{t})p(x_{t}\st x_{t-1})	, \label{particle filter OID}}
which we could also have obtained from applying \eqref{optimal proposal}. A skeleton of the algorithm is given below and produces a weighted set of particles $\{X^{(i)}_{1:t},w^{(i)}_{1:t}\}_{i=1}^{N}$ with corresponding empirical distribution $\hat \pi_{t}(x_{1:t})$. 
%
%
\begin{algorithm}[!h]\small
	\caption{\dblue{\emph{\small Skeleton of a basic particle filter}}}
	\begin{algorithmic}[1]
		\State sample $X_{1}^{(i)}\sim q_{1}(\cdot)$ %for $i=1,\dots,N$	%\Comment{\emph{Initialization}}
		\State compute the weights $w_{1}(X^{(i)}_{1})\propto {\mu( X^{(i)}_{1})p(y_{1}\st X^{(i)}_{1})/ q_{1}(X^{(i)}_{1})}$
%		\State resample: $\{\X^{(i)}_{1},W^{(i)}_{1}\}\rightarrow\{\overline \X^{(i)}_{1},N\inv\}$\Comment{\emph{Using some resampling scheme}}\vspace*{.2cm}
		\For{$t=2:T$}
			\State sample $X^{(i)}_{t}\sim q_{t}( \cdot \st X^{(i)}_{t-1})$ with $q_{t}\approx q_{t}^{\text{opt}}$
			\State update and normalise the weights $w^{(i)}_{t}\propto\alpha^{(i)}_{t}w^{(i)}_{t-1}$
		\EndFor
	\end{algorithmic}
\end{algorithm}

\noi\hspace{0pt}\margnote{PF complexity}The complexity of the above algorithm is $\mathcal O(TN)$. Indeed, at each time step $t$, one needs to sample $N$ particles and compute their corresponding weights which has linear complexity in the number of particles.\\

\noi\textbf{Remark}: it is often difficult to sample directly from the optimal proposal. In the context of filtering, a frequently used alternative is the \margnote{bootstrap prop.}\emph{bootstrap proposal} with 
\eqa{q^{\text{bs}}(x_{t}\st x_{t-1})&:=&p(x_{t}\st x_{t-1}),}
which is often easier to sample from. In that case, the update factor simply reduces to $\alpha^{(i)}_t \propto p(y_t\st X^{(i)}_t)$. Since the likelihood and the transition density may not be aligned, those factors can vary a lot with, as a consequence, an increased variance of the resulting estimator.\\


Another observation is that, in the above algorithm, we have omitted the \emph{resampling}\margnote{resampling} step whereby particles can be resampled with replacement based on their weights. In practice, this alleviates the problem of variance growth over time when one considers a suboptimal proposal distribution such as the bootstrap proposal. Although this step is a key aspect of particle filters, it is not an aspect that plays an important role in the issues we consider in this document; in the sequel we will assume that a standard multinomial resampling is applied. \\

For a more extensive review of particle filtering we refer to \cite{doucet11} and references therein. In the rest of the document, we will assume that a good sequence of empirical densities approximating the filtering densities has been built and will focus on the modification of that sequence leading to estimators of the smoothing densities.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Particle smoothing}

