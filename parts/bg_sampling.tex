% !TEX root = ../thesis.tex

In this chapter, we start with a brief overview of Monte Carlo estimators and associated sampling methods focusing in particular on Sequential Monte Carlo methods for the smoothing and filtering problem on Hidden Markov Models. We also introduce an alternative class of methods relying upon Piecewise Deterministic Markov Processes and in particular the Bouncy Particle Sampler (BPS). 

\section{\label{sec:MC+SMC}Monte Carlo methods}
In this section we review briefly the foundations of Monte Carlo methods as well as a few key algorithms which will be discussed in the rest of the document.

\subsection{From quadrature to sampling}
% KEEP pi (later p is filtering/smoothing)
We consider the problem of computing the expected value of a test function $\varphi$ taking value over a space $\mathcal X$ with respect to a distribution $\pi\in\mathcal P(\mathcal X)$ i.e.: $I:=\E_\pi[\varphi(X)]$. Assuming it can't be computed analytically, a general approach is to consider a quadrature rule of the form:
%
\eqa{
	I\esp\approx\esp \widehat I_N &=& \sum_{i=1}^{N} w(X^{(i)}) \varphi(X^{(i)})
}
%
for some fixed points $X^{(i)}\in\mathcal X$ and corresponding weights $w(X^{(i)})\in\mathbb R_+$. 
When the number of dimensions is low, we can consider deterministic quadrature rules such as the Gauss-Hermite quadrature (see e.g.: \citet{davis75}). However, as the dimensionality increases, the performance of these deterministic rules becomes catastrophic even for a large number of integration points (a well-known effect related to what Bellman called the \emph{curse of dimensionality} in dynamic programming \citep{bellman57,bengtsson08}). 
In such cases, a broadly studied approach is the Monte Carlo integration with
%
\eqa{	
	X^{(i)}\esp \simiid\esp \pi,\quad\text{and}\quad w(X^{(i)})\spe N\inv. 	
\label{eq:mcsampling}}
%
The strong law of large numbers then indicates that $\widehat I_N\to I$ almost surely with approximation error scaling like $\mathcal O(N^{-1/2})$ independently of the dimensionality of the problem. This is to be compared with deterministic rules which typically have approximation error scaling like $\mathcal O(N^{-\alpha/d})$ for a fixed $\alpha>0$ depending on the quadrature rule \citep{caflisch98}. The problem then becomes one of drawing iid.\ samples from $\pi$ which is often also an intractable problem. Note that \eqref{eq:mcsampling} in fact defines an \emph{empirical density} $\hat \pi$ with
%
\eqa{
	\hat \pi(x) &:=& N\inv\sum_{i=1}^{N} \delta_{X^{(i)}}(x),
}
%
and computing $\hat I_{N}$ amounts to taking the expected value of $\varphi(X)$ with respect to $\hat\pi$.\check{paragraph jul16}

%%%%%%%%%%%%%%%%%%%
\subsection{Importance sampling}
%For this point, we refer to the note by \citet{doucet11} and also to \citet[chapter 3.3 and 14.3]{robert04}.
In \emph{importance sampling} (IS), samples are drawn from a \emph{proposal distribution} $q\in\mathcal P(\mathcal X)$ that is easy to sample from (e.g.: a Gaussian) and is similar to the target distribution $\pi$. 
The quadrature weights are then adjusted to reflect that the samples are not drawn from the true distribution:
%
\eqa{
	X^{(i)}\esp \simiid\esp q(\cdot), \quad\text{and}\quad w(X^{(i)})\esp\propto\esp {\pi(X^{(i)})\over q(X^{(i)})}.\label{eq:impsampling}
} 
%
Provided the support of $q$ includes that of $\pi$ i.e., $\pi(x)>0\Rightarrow q(x)>0$, the resulting \emph{importance sampling estimator} is consistent. Further, the estimator of the expected value of a specific test function $\varphi$ is finite provided $\E_{\pi}[\varphi(X)^{2}w(x)]<\infty$ \citep[chapter 3.3]{robert04}. Note that \eqref{eq:impsampling} also defines an empirical density $\hat\pi$ with
%
\eqa{
	\hat\pi(x) &=& \sum_{i=1}^{N} w(X^{(i)})\delta_{X^{(i)}}(x)
}
%
with the weights $w(X^{(i)})$ summing to one. 

%%%%%%%%%%%%%%%%%%%
\subsubsection{Effective sample size}
One way to assess the quality of an importance sampling estimator is to consider the ratio of the variance of the corresponding Monte Carlo estimator and the variance of the IS estimator. The general ratio is hard to handle and depends upon the test function with respect to which the expected value is computed. \citet{kong92} suggested considering the following proxy known as the \emph{effective sample size}:
%
\eqa{
	\text{ESS} &=& {N\over 1+\V_{q}(W)},
}
%
where $\V_{q}(W)$ is the variance of the importance sampling ratio. Using the sample variance of the normalised weights and simplifying the expression, the proxy that is usually considered for the ESS nowadays is
%
\eqa{
	\text{ESS} &=& \pat{\sum_{i=1}^{N}w(X^{(i)})^{2}}^{-1}\label{eq:ESS}
}
%
This metric is bounded from below by $1$ -- the degenerate case where a single particle is carrying all the weight -- and from above by $N$ -- the ideal case where the samples are comparable to ideal Monte Carlo samples. \check{par jul16}

\subsection{Classical sampling algorithms}

In this point we mention briefly a few standard algorithms used to attempt to generate samples from a distribution.
Later on, we will compare alternative methods to these standard algorithms.
The list and descriptions are not meant to be exhaustive and we refer to \citep{robert04, green15} for a more complete overview.

\subsubsection{Metropolis-Hasting}
The Metropolis-Hasting algorithm belongs to the class of Markov Chain Monte Carlo methods (MCMC) where one attempts to generate a sequence of random variables $X^{(1)},X^{(2)},\dots$

\subsubsection{Gibbs sampling}
\subsubsection{Hamiltonian Monte Carlo sampling}

%%%%%%%%%%%%%%%%%%%%%%
\section{Sequential Monte Carlo methods}
%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sequential importance sampling}
%%%%%%%%%%%%%%%%%%%
\subsubsection{Hidden Markov models}
In the context of HMM, we are usually interested in estimating a sequence of target distributions $\{\pi_{t}(x_{1:t})\}_{t=1}^{T}$ which admit the following factorisation structure:
%
\eqa{
	\pi_{t}(x_{1:t}) &=& \pi_{t}(x_{t}\st x_{1:t-1})\pi_{t-1}(x_{1:t-1}).\nn
}
%
This lead to the development of \emph{sequential Monte Carlo} (SMC) methods \citep[chapter 14.3]{robert04}. The underlying principle of sequential importance sampling is the same as that of importance sampling except that a different proposal is considered at every step $t$ taking into account the previous draw of particles and the evolution of the system:
%
\eqa{
	q_{t}(x_{1:t}) &=& q_{t}(x_{t}\st x_{1:t-1})q_{t-1}(x_{1:t-1}).	\nn
}
%
Following this form, new samples or \emph{particles} $X^{(i)}_t$ can be drawn from $q_{t}(x_{t}\st X^{(i)}_{1:t-1})$ and the weights corresponding to the trajectories $\{X^{(i)}_{1:t}\}$ then need to be updated by a factor $\alpha^{(i)}_{t}$ with\check{jul16, jun24}
%
\eqa{
	\alpha^{(i)}_{t} := {\pi_{t}(X^{(i)}_{1:t} )\over \pi_{t-1}(X^{(i)}_{1:t-1}) q_{t}(X^{(i)}_{t}\st X^{(i)}_{1:t-1})}.
}
%
The variance of an IS estimator is directly related to the variance of the associated importance weights. In order to counter the increase of variance induced by the sequential IS procedure, the proposal at step $t$ should be such that the variance of the update factors $\alpha_t$ is as small as possible. In particular, the \emph{optimal proposal} \citep{doucet11} keeps it at zero with
\eqa{		q^{\text{opt}}_{t}(x_{t}\st x_{1:t-1}) &:=& \pi_{t}(x_{t}\st x_{1:t-1}).	\label{optimal proposal}}
Note that "optimality" here is understood in terms of the variance of the estimator. Additionally, since we can't typically sample easily from the optimal proposal, we have to resort to approximating distributions.\check{jul16, jun24}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Particle filtering}
In the filtering problem on a HMM, the target densities are $\pi_{t}(x_{1:t})=p(x_{1:t}\st y_{1:t})$ and their marginals. The incremental update factors are given by
\eqa{ \alpha_t(x_{1:t}) &=& {p(x_{1:t}\st y_{1:t})\over p(x_{1:t-1}\st y_{1:t-1})q_t(x_t\st x_{1:t-1})} 	\nn\\
	&=& {p(x_t,y_t\st x_{1:t-1},y_{1:t-1})p(x_{1:t-1}\st y_{1:t-1})p(y_{1:t-1})\over p(x_{1:t-1}\st y_{1:t-1})q_t(x_t\st x_{1:t-1})p(y_{1:t})}  \nn\\
	&\propto& {p(y_t\st x_t)p(x_t\st x_{t-1}) \over q_t(x_t\st x_{1:t-1})},}
where we exploited the conditional dependence structure of a HMM. Consequently, the optimal proposal is
\eqa{		q^{\text{opt}}_{t}(x_{t}\st x_{t-1}) &\propto& p(y_{t}\st x_{t})p(x_{t}\st x_{t-1})	, \label{particle filter OID}}
which could also have obtained from applying \eqref{optimal proposal}. A skeleton of a particle filter algorithm is given below.\check{jun24}
%
\begin{algorithm}[!h]\small
	\caption{\label{alg:particle-filter}\dblue{\emph{\small Particle filter}}}
	\begin{algorithmic}[1]
		\State sample $X_{1}^{(i)}\simiid q_{1}$ for $i=1,\dots,N$	%\Comment{\emph{Initialization}}
		\State compute and normalise the weights $w_{1}(X^{(i)}_{1})\propto {\pi_{0}( X^{(i)}_{1})p(y_{1}\st X^{(i)}_{1})/ q_{1}(X^{(i)}_{1})}$
%		\State resample: $\{\X^{(i)}_{1},W^{(i)}_{1}\}\rightarrow\{\overline \X^{(i)}_{1},N\inv\}$\Comment{\emph{Using some resampling scheme}}\vspace*{.2cm}
		\For{$t=2:T$}
			\State sample $X^{(i)}_{t}\simiid q_{t}( \cdot \st X^{(i)}_{t-1}, y_{t})$ with $q_{t}\approx q_{t}^{\text{opt}}$ for $i=1,\dots,N$
			\State update and normalise the weights $w^{(i)}_{t}\propto\alpha^{(i)}_{t}w^{(i)}_{t-1}$
		\EndFor\\
		\Return weighted set of particles $\{X^{(i)}_{1:T},w^{(i)}_{1:T}\}_{i=1}^{N}$
	\end{algorithmic}
\end{algorithm}
%

The computational complexity of algorithm \ref{alg:particle-filter} is $\mathcal O(TN)$. Indeed, at each step $t$,  the algorithm samples $N$ particles and computes their corresponding weights which has linear complexity in the number of particles.\check{jun24}

Sampling directly from the optimal proposal is often an intractable problem by itself. In the context of filtering, an alternative choice is the \emph{bootstrap proposal} \citep{doucet11} with
\eqa{q^{\text{bs}}(x_{t}\st x_{t-1})&:=&p(x_{t}\st x_{t-1}),}
which is often easier to sample from. In that case, the update factor simply reduces to $\alpha^{(i)}_t \propto p(y_t\st X^{(i)}_t)$. However, since the likelihood and the transition density may not be well aligned, those update factors can vary a lot incurring an increase in the variance of the resulting estimator.\check{jul16,jun29}

We have omitted the \emph{resampling step} in algorithm \ref{alg:particle-filter}. That step resamples particles with replacement based on their weights if the ESS comes under a pre-assigned threshold \citep{delmoral06}. In practice, this alleviates the problem of weight degeneracy incurring growth of variance over time when one considers a suboptimal proposal distribution such as the bootstrap proposal. Although this step is a key aspect of particle filters, it is not an aspect that plays an important role in the issues we consider in this document; in the sequel we will therefore assume that a standard multinomial resampling is applied \citep{hol06,doucet11}.\check{jul16}

\subsection{\label{bg:particle-smoothing}Particle smoothing}

In the smoothing problem on a HMM, the target densities are $p(x_t\st y_{1:T})$. These densities can be expressed as the marginals of joint densities over subsequent states: 
\eqa{p(x_t\st y_{1:T})&=&\int p(x_t,x_{t+1}\st y_{1:T})\,\mathrm{d}x_{t+1}.\nn} Exploiting the conditional dependence structure of the HMM, the integrand can be factorised leading to
\eqa{	p(x_t\st y_{1:T}) &=& \int p(x_t\st x_{t+1},y_{1:t})p(x_{t+1}\st y_{1:T})\,\mathrm{d}x_{t+1}	\nn\\&=& p(x_t\st y_{1:t}) \int {p(x_{t+1}\st x_t)\over p(x_{t+1}\st y_{1:t})}p(x_{t+1}\st y_{1:T}) \,\mathrm{d}x_{t+1}.\label{eq FFBS}}
This relation leads to the \emph{forward filtering, backward smoothing} (FFBS) algorithm \citep{hurzeler98, doucet00}.
Plugging the particle approximation to the filtering distribution in equation \eqref{eq FFBS} gives:
\eqa{		
	\hat{p}(x_{t}\st y_{1:T}) &=& \hat{p}(x_{t}\st y_{1:t})\int {p(x_{t+1}\st x_{t})\over \int p(x_{t+1}\st x_{t})\hat p(x_{t}\st y_{1:t})  \dx_{t}}\hat p(x_{t+1}\st y_{1:t+1})\dx_{t+1}\nn \\
			&= &	\sum_{i=1}^{N}w^{(i)}_{t\st T}\delta_{X^{(i)}_{t}}(x_{t}),	}
where the smoothing weights are given recursively by
\eqa{		w^{(i)}_{t\st T} &\propto& w^{(i)}_{t}\sum_{j=1}^{N}{w^{(j)}_{t+1\st T}\pac{p(X^{(j)}_{t+1}\st X^{(i)}_{t})\over \sum_{k=1}^{N}w^{(k)}_{t}p(X^{(j)}_{t+1}\st X^{(k)}_{t})}}. \label{eq FFBS weights}	}
In essence, the FFBS algorithm simply recycles a particle filter updating its weights according to equation \eqref{eq FFBS weights}. \check{jul16}

The computation of the updated smoothing weights has complexity $\mathcal O(TN^{2})$ since, at each step, we need to consider the matrix of all pairwise interactions between the particles at two subsequent steps: $[p(X^{(j)}_{t+1}\st X^{(i)}_{t})]_{i,j=1}^{N}$. Since the FFBS algorithm recycles the particles from a particle filter, the performances of the resulting estimators can suffer if the support of the filtering distribution at step $t$ is significantly distinct from that of the smoothing distribution at step $t$.\check{jul16, jun29,jun24}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\label{point:BPS}Sampling with Piecewise Deterministic Markov Processes}
The \emph{Bouncy Particle Sampler} (BPS) of \citet{bouchard15} is inspired from an algorithm in the physics literature \citep{peters12} and belongs to a wider class of sampling methods based on Piecewise Deterministic Markov Processes (PDMP) \citep{bierkens16, bierkens17, wu17}.

The BPS algorithm considers a target distribution $\pi$ on $\R^{d}$, proportional to a non-negative function $\gamma$ which can be evaluated pointwise: $\pi(x) = Z\inv \gamma(x)$
but the normalisation constant $Z$ is intractable. 
The objective is again the computation of expected values of the form $\E_\pi[\varphi]$ for an arbitrary test function $\varphi:\R^{d}\to \R$. 

Defining the \emph{energy} function $U$ as $U(x) = -\log \gamma(x)	$
 and assuming it is continuously differentiable, the algorithm generates a piecewise linear path based on $\nabla U$. 
 Each segment is specified by an initial position $x^{(i)}\in \R^{d}$, a length $\tau_{i+1}\in \R^{+}$ and a velocity $v^{(i)}$ with the recurrence $x^{(i+1)}=x^{(i)}+v^{(i)}\tau_{i+1}$. The times where the velocity changes are given by the cumulative sum, $t_i=\sum_{j=1}^{i}\tau_j$ with $t_0=0$. The continuous positions along the path are therefore given by
\eqa{	x(t) &=& x^{(i)} + v^{(i)}(t-t_i), \quad\text{for}\quad t\in[t_i,t_{i+1}).	\label{eq:path-BPS}}
The authors show that, if the lengths $\{\tau_{i}\}_{i\ge 1}$ are governed by a specific Inhomogenous Poisson Process (IPP) and if the speeds are modified for each segment according to a simple reflection mechanism, then the target expected values can be approximated consistently via an integral of the test function along the path given by \eqref{eq:path-BPS}.

The IPP governing the lengths has intensity function $\lambda:\R^{d}\times\R^{d}\to\R^{+}$ with
\eqa{	\lambda(x,v) &=& \max\,\{0,\scal{\nabla U(x),v}\}.	}
At the end of each segment, the trajectory ``bounces'' and the velocity is reflected against the local level set of $U$:
\eqa{	v' &=& R(x)v \spe v-2{\scal{\nabla U(x),v}\nabla U(x)\over \bnorm{\nabla U(x)}^{2}}.	}
The authors show that the velocities also need to be ``refreshed'' according to the arrival times of a homogenous PP of intensity $\lambda^{\text{ref}}\ge 0$, a parameter of the BPS. The basic BPS algorithm is reproduced in \hyperref[alg:BPS1]{algorithm~\ref*{alg:BPS1}} below.

\begin{algorithm}[!h]\small
	\caption{\label{alg:BPS1}\small \idblue{Basic BPS}}
	\begin{algorithmic}[1]
	\State init $(x^{(0)},v^{(0)})$, set $T$, trajectory length, set $t=0$ and $i=1$
	\While{$t < T$}
		\State simulate first arrival time $\tau_{\text{bounce}} \sim \text{PP}(\chi(t))$  with
		$$ \chi(t) = \lambda(x^{(i-1)}+v^{(i-1)}t,v^{(i-1)})$$
		\State simulate $\tau_{\text{ref}}\sim \mathrm{Exp}(\lambda^{\text{ref}})$
		\State let $\tau_{i}\leftarrow \min(\tau_{\text{bounce}},\tau_{\text{ref}})$
		\State compute the next position $x^{(i)}\leftarrow x^{(i-1)}+v^{(i-1)}\tau_{i}$
		\If{$\tau_{i}=\tau_{\text{ref}}$}
			\State sample next velocity $v^{(i)}\sim \mathcal N(0,I)$		\EndIf
		\If{$\tau_{i}=\tau_{\text{bounce}}$}
			\State reflect the velocity $v^{(i)}\leftarrow R(x^{(i)})v^{(i-1)}$
		\EndIf
		\State let $t\leftarrow t_{i}\leftarrow t_{i-1}+\tau_{i}$
		\State update $i\leftarrow i+1$
	\EndWhile
	\end{algorithmic}
\end{algorithm}

The expectations can then be approximated using
\eqa{	\E_\pi[\varphi] &=& T\inv \int_0^{T} \varphi(x(t))\mathrm{d}t	\nn\\
&\approx& T\inv \pat{\sum_{i=1}^{n-1} \int_0^{\tau_i} \varphi(x^{(i-1)}+v^{(i-1)}s)\,\mathrm{d}s + \int_0^{t_n-T}\varphi(x^{(n-1)}+v^{(n-1)}s)\mathrm{d}s}, \nn}
where the integrals in the sum can themselves be approximated if needed by quadrature.

As we saw, the algorithm requires to sample the first arrival time of an IPP of intensity $\chi(t)=\lambda(x+vt,v)$ where $\lambda(x,v)=\max\{0,\scal{\nabla U(x),v}\}$. Following existing literature, the authors suggest three possible methods which we list briefly below:
\begin{itemize}\itsep0
    \item \textbf{time-scale transformation}: this approach requires computing the inverse quantile of $\int_{0}^{t} \chi(s)\mathrm{d}s$ which is usually intractable. However, it can be done numerically if the target is strictly log-concave and differentiable. %This, the authors show, allows to make a connection with the Metropolis Hastings algorithm.
    \item \textbf{adaptive thinning}: this approach requires an upper bounds $\overline{\chi}_{s}(t)$ with $\overline\chi_{s}(t)=0$ for $t<s$ and $\overline\chi_{s}(t)\ge\chi(t)$ for $s\le t\le s+\Delta(s)$ where $\Delta$ is a positive function (in the standard case, $\Delta=+\infty$). Additionally, it requires the ability to sample from the IPP corresponding to $\overline\chi_{s}$. Ideally, $\Delta$ and $\chi/\overline\chi_{s}$ are large (so that we don't have to simulate too many candidates which incurs a computational cost).
    \item \textbf{superposition}: if the energy can be decomposed in a sum and that each term can be targeted (via one of the first two methods).
\end{itemize}

%\section{Discussion}
%
%\todofr{
%\textbf{move this discussion where it belongs: at end of content chapters}
%	Connect the two background sections with main element (PF, PS link to HMM, PDMP, Local PDMP for anything. Exact methods.).
%}


