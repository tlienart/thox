% !TEX root = ../thesis.tex

In this chapter, we offer a critical look on the results obtained in this thesis and suggest further lines of work.

The purpose of this thesis was to leverage factorisation structure in probabilistic models in order to reduce the computational cost associated with traditional Bayesian inference methods. While we obtained some interesting results improving existing methods, it became clear during the analysis that some of the foundations of the methods themselves were flawed. We discuss this superficially below before elaborating for each of the approaches we considered of the thesis. 

A first simple criticism that can be leveraged towards performing inference on MRF is that the inference step is, in some sense, a secondary one. The first and arguably most important one is to determine the model itself. In terms of the MRF this corresponds to determining the topology of the graph and determining the potentials on the cliques. This choice is potentially much more important than the inference algorithm used thereafter and discussing accuracies obtained by computational methods can seem inappropriate when the uncertainty around the model choice overwhelms the accuracy improvements obtained. 

A second key criticism is that one of the justification for attempting to perform Bayesian inference is to model uncertainty. However it became clear during the work that led to this document that while the literature very often mentions the need for uncertainty estimates, it equally often forgets to mention how the quality of this uncertainty is assessed. Further, much of the literature in ``Bayesian Machine Learning'' discusses accuracy metrics such as the predictive RMSE or other metrics obtained with the posterior mean. This is odd since optimisation methods relying on an appropriately regularised maximum-likelihood estimator will lead to similar (and likely better) metrics at a fraction of the computational cost using optimisation methods (see also \citep{green15}). %Highly cited examples abound from \cite{hoffman13}
Finally, using surrogate distributions as has been popularised with computationally-cheap methods such as \emph{stochastic variational inference} \citep{hoffman13} often leads to uncertainty estimates that are inadequate as discussed by \cite{wang05}.

\section{On Sampling Methods}

In the part on sampling methods, we started by considering the smoothing problem on a well defined non-linear Hidden Markov Model. Standard approaches such as the Forward Backward Smoothing Algorithm require a quadratic computational costs in the number of particles used to represent the marginals. We showed that these methods do not suffer when colliding populations of samples are subsampled to reduce the total number of interactions that need to be considered. While this is a good news in terms of computational complexity, we can't help but note that the overall performance of all particle smoothers we tested (quadratic or not) is rather poor at recovering a signal close to the ground-truth and, more importantly, does not seem empirically to offer a significantly better approximation than simply looking at a Particle Filter. This point becomes even more obvious if the transition and observation densities are not known precisely.

We then looked at applying the Local Bouncy Particle Sampler on the Probabilistic Matrix Factorisation problem. While the application was interesting in its own right and thereby showed that the LBPS seems to be a viable option for inference on large graphical models, the reader surely noticed that the reported results -- RMSE on the test set, the metric of choice of the corresponding literature -- were blown out of the water by an adequate application of the Sparse SVD algorithm. This may reflect the data we considered (MovieLens 1M) since \cite{mnih08} show better performances of Bayesian PMF over SVD.\footnote{Though it is important to note here that it is not specified \emph{what} implementation of SVD they used nor whether they centred the data before applying the SVD. Applying the SVD on uncentred data is sure to lead to a biased results for obvious reasons.} The LBPS is still in its early days of development and future work could look in more details at the selection of parameters such as the refreshment rate $\lambda$ which seem to play an important role in the quality of the output trajectories. Exploring and interpreting results obtained on other large scale graphical models would also add to the understanding of this interesting method.

\section{On Approximate Bayesian Inference}

In the part on Approximate Bayesian Inference, we presented versions of the Expectation Propagation (EP) algorithm which are robust to Monte Carlo noise and can therefore be used as the backbone of a distributed Bayesian inference mechanism. We showed that a version of the EP algorithm inspired from the Mirror Descent algorithm performs much better than other versions considered. This is a positive result however it is unclear whether the uncertainty estimates obtained with the procedure are usable in complex models. Further, since (as per usual) it is the predictive RMSE that is reported, more work should be done to compare the results obtained from applying such a method as compared to a optimisation-based methods such as \emph{downpour SGD} \citep{dean12}.\footnote{Note that this was partially done in our paper \citep{hasenclever16} but should be repeated in a simpler setting where all the moving parts are appropriately controlled.} 

Lastly, we looked at improving on the Particle Belief Propagation algorithm for arbitrary undirected graphical models using proposals on graph nodes that were adaptively constructed using EP. The method clearly outperformed PBP and is computationally not too expensive making it an attractive alternative. However, the method assumes that performing the Loopy Belief Propagation algorithm is an appropriate way of recovering sensible marginals on the graph nodes which is not guaranteed. The recovered beliefs may be good proxies for marginals but there are no guarantees as how they compare to the true marginals.

\section{On scalable Bayesian Methods}

Recently there has been much interest in applying Bayesian methods for in the ``Big Data'' setting as well as in Machine Learning. We refer here to two excellent reviews on the topic \citet{green15, bardenet17} both leading to a rather negative perspective on existing methods and approaches in the field. 

Let us consider classical Machine Learning models based on a feature matrix of dimensions $n \times p$ where $n$ is the number of instances and $p$ the number of dimensions of the feature space (state space). In the case of low $n$ (hundreds or less) and low $p$ (half a dozen or less), Bayesian computations are known to be computationally practical and useful \citep{gelman13}. In the case of large $n$ and $p\ll n$, also known as \emph{tall data}, recent work has shown that Bayesian inference methods are not particularly useful and may in fact underperform significantly \citep{bardenet17, nagapetyan17}. However, the case of increasing interest has $p$ much larger than half a dozen. The question then is the kind of conditional dependence structure that is assumed on the features. Assuming a fully dependent model (dense graph) often leads to computationally intractable problems. Assuming a fully independent model (disconnected graph) or proxy like in Mean-Field Variational Inference or EP with a diagonal Gaussian exponential family may be computationally cheap but may also be inappropriate and lead to grossly underestimated uncertainty estimates.  Further, a better approach in the case where one is willing to consider a disconnected graph may simply be to consider a regularised maximum likelihood estimator. Lastly, assuming a sparse graphical model (sparsely connected graph) appears to us to be the case in which Bayesian methods have the most chance of offering good results but few inference methods truly leverage such a structure though the LBPS is a promising first attempt. We believe that this a relatively ill-explored area of Bayesian computations where more work could lead to interesting methods and modern applications.
