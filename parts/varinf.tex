% !TEX root = ../thesis.tex

In this thesis we define approximate Bayesian inference in broad terms as the class of methods attempting to recover a distribution $q$ in a restricted family of distributions $\mathcal F\subset \mathcal P(\mathcal X)$ such that $q$ is a ``good'' proxy for a posterior distribution $p$ of interest. 
This definition leaves significant room to describe different methods based on the definition of what a ``good proxy'' means. 

Let $D:\mathcal P(\mathcal X)\times \mathcal P(\mathcal X)\to \mathbb R_{+}$ denote a divergence between two probability distribution functions. Then, the generic variational problem we consider is the minimisation of this divergence in $\mathcal F$:
%
\eqa{
q^{\star} &\in& \arg\min_{q\in \mathcal F}\quad D(q,p).\label{eq:generic-abi}
}
%
Techniques attempting to solve such problems rely upon exploiting at least one of the three main characteristics: 
\begin{itemize}\itsepa
\item the definition of the discrepancy measure $D$, 
\item the definition of the restricted space $\mathcal F$, and
\item the structure of the target distribution $p$. 
\end{itemize}
Many discrepancy measures can be considered such as the total variation distance, the Wasserstein distance or $f$-divergences \citep{minka04, blei16, li16, bernton17}. However, most choices lead to the corresponding problem \eqref{eq:generic-abi} being computationally very expensive to solve in general especially when the space $\mathcal X$ is continuous. In this part we. focus on the Kullback-Leibler divergence \citep{kullback51} with $\KL{p,q}:=\E_{p}[\log (p(X)/q(X))]$.

%%%%%%%%%%%%%%%%%
\subsection{Variational inference}
The KL divergence has been widely considered in the literature partly because it leads to variational problems that are amenable to gradient descent-based algorithms. 
In particular, considering the discrepancy $\KL{q,p}$ lead to the development of the popular \emph{variational inference} algorithms \citep{hoffman13,blei16}. 

In \emph{mean-field variational inference} (MFVI), the approximating family of distributions $\mathcal F$ is taken to be the class of distributions that fully factorises:\add{fully is not necessarily required, should probably add note, should probably also actually write the stuff down}
%
\eqa{
	\mathcal F_{\text{MFVI}} &=& \pab{q \in \mathcal P(\mathcal X) \st q(x_{1},\dots,x_{d}) = \prod_{i=1}^{d}q_{i}(x_{i}) }.
}
% 
This choice of distribution space, while rather restrictive, leads to the inference problem \eqref{eq:generic-abi} being tractable: indeed, writing $q_{\neg i}(x_{\neg i}) = \prod_{j\neq i} q_{j}(x_{j}) $, we have
\eqa{
	\KL{q,p} &=& \E_{q_{i}}\pac{ \log q_{i} -	\E_{q_{\neg i}}\pac{ \log p}} + \E_{q_{\neg i}}\pac{\sum_{j\neq i}\log q_{j}} 
}
suggesting to iterate over the components taking each time $q_{i} \equiv \exp(\E_{q\neg i} \log p) $. This scheme can be used to provably decrease the objective function \citep{hoffman13, kucukelbir16, blei16}. However, the problem is non-convex and no practical guarantees exist as to what the iterations converge to. 
In this thesis, we focus on an alternative approximate Bayesian inference class of algorithms which exploits the factorisation structure of the target distribution $p$ directly instead of imposing a factorisation structure a priori like in MFVI.

\subsection{ADF, EP and LBP}
The Assumed Density Filtering (ADF) algorithm and the  Expectation Propagation (EP) algorithm are two other popular methods that are also based on the KL divergence. The choice of distribution space is usually the exponential family associated with a sufficient statistic $\phi$. Additionally, these methods assume that the target distribution $p$ factorises in terms that are easier to handle than $p$ itself. We introduce these methods at points \ref{point:expof-convex} and \ref{s:ADF+EP}.

The Belief Propagation (BP) algorithm and the Loopy Belief Propagation (LBP) algorithm are message-passing algorithms which target distributions that factorise according to a MRF. Those methods can also be shown to be fixed-point algorithms corresponding to a specific form of \eqref{eq:generic-abi} \citep{yedidia01, yedidia02}.

In this part of the thesis we focus on the use of EP and (L)BP algorithms for inference on MRF and discuss applications. In point \ref{s:ADF+EP} we cover the ADF and EP algorithms with exponential family distributions and in point \ref{bg:belief-propag} we introduce the BP and LBP algorithms.




